# -*- coding: utf-8 -*-
"""DSSM_recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BzIQcSmGXKr_p2NGkosjghKcwE0zunTN
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

import tensorflow.compat.v1 as tf
import numpy as np
from tqdm import tqdm
from tensorflow.keras.initializers import glorot_uniform
import sys
from tensorflow.keras.losses import categorical_crossentropy

tf.disable_v2_behavior()
"""# Visualize Data"""

max_len = 20
hidden_size = 128
#no_items = 229227
#no_user = 145055
user_emb_size = 16
item_emb_size = 32
batch_size = 512
epochs = 128000

import pandas as pd

df = pd.read_json('recommand_data.json')

df = df.drop(df[df['videoids'].map(len) > 50].index)

df = df.tail(100)
user_list = []
for i in df['msisdn'].values.tolist():
    user_list.append(i)
no_user = len(list(set(user_list)))
print("Number of unique user : {}".format(no_user))
user_dictionary = {}
for pos,user in enumerate(list(set(user_list))):
    user_dictionary[user]=pos

item_list = []
max_session = 0
for i in df['videoids'].values.tolist():
    if len(i) > max_session:
        max_session = len(i)
    if len(i) == 167860:
        print("Max_session_length : {}".format(len(set(i))))
        #print("Session : {}".format(i))
    for j in i:
        item_list.append(j)
no_items = len(list(set(item_list)))
item_dictionary = {}
for pos,item in enumerate(list(set(item_list))):
    item_dictionary[item]=pos
print("Number of unique items : {}".format(no_items))

# print("Item dictionary : {}".format(item_dictionary))

def split_session(session_id,session,max_session_length,user_dictionary,item_dictionary):
    """
    Session_id : List user for the session. (List) -> convert to user_emb position.
    Session : List of items for session -> convert to items_emb position.
    Max_session_length : maximum session length to split
    """
    list_session = []
    list_label = []
    list_id = []
    for x,j in zip(session_id,session):
        temp = [item_dictionary[k] for k in j]
        for i in range(1,len(temp)):
            list_id.append(user_dictionary[x])
            list_label.append(temp[i])
            if i-max_session_length < 0:
                list_session.append(temp[:i])
            else:
                list_session.append(temp[i-max_session_length:i])
    for i in list_session:
        while len(i) < max_session_length:
            i.insert(0,0)
    list_session = np.asarray(list_session)
    list_label = np.asarray(list_label)
    list_id = np.asarray(list_id)
    return list_id,list_session,list_label

list_id,list_session,list_label = split_session(df['msisdn'].values.tolist(),df['videoids'].values.tolist(),max_len,user_dictionary,item_dictionary)

print("ID shape : {}".format(list_id.shape))
print("Session shape : {}".format(list_session.shape))
print("Label shape : {}".format(list_label.shape))

# from time import time
# user_test = np.random.randint(low=0,high=no_user,size=1)
# item_test = np.random.randint(low=0,high=no_items,size=(1,max_len))
# print("User {}".format(user_test))
# print("Item set : {}".format(item_test))
# t = time()
# print("Prediction : {}".format(sess.run([prediction],feed_dict={user_id:user_test,item_session:item_test})))
# print("total time = {}".format(time()-t))

def get_one_hot(targets, nb_classes):
    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]
    return res.reshape(list(targets.shape)+[nb_classes])

user_id = tf.placeholder(tf.int32,shape=(None,))
item_session = tf.placeholder(dtype=tf.int32,shape=(None,None))
y = tf.placeholder(dtype=tf.float32,shape=(None,no_items))
user_emb = tf.get_variable("user_emb",shape=(no_user,user_emb_size),initializer=glorot_uniform)
item_emb = tf.get_variable("item_emb",shape=(no_items,item_emb_size),initializer=glorot_uniform)   
embedded_user = tf.nn.embedding_lookup(user_emb, user_id)
embedded_items = tf.nn.embedding_lookup(item_emb,item_session)

embedded_average = tf.reduce_mean(embedded_items,1)
embedded_concat = tf.concat([embedded_user,embedded_average],axis=1)
weight_dense_1 = tf.get_variable("w1",shape=(user_emb_size+item_emb_size,64))
bias_dense_1 = tf.get_variable("b1",shape=(64,))
dense_1 = tf.nn.relu(tf.matmul(embedded_concat,weight_dense_1)+bias_dense_1)
weight_dense_2 = tf.get_variable("w2",shape=(64,no_items))
bias_dense_2 = tf.get_variable("b2",shape=(no_items,))
dense_2 = tf.nn.softmax(tf.matmul(dense_1,weight_dense_2)+bias_dense_2)
loss = tf.reduce_mean(categorical_crossentropy(y,dense_2))
optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)
prediction = tf.argmax(dense_2,1)
accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,tf.argmax(y,1)), "float"))

# def get_batch(batch_size=3):
#   user = np.random.randint(low=0,high=no_user,size=batch_size*64)
#   item = np.random.randint(low=0,high=no_items,size=(batch_size*64,max_len))
#   y_train = []
#   list_label = []
#   for i in range(batch_size*64):
#       temp = np.random.randint(low=0,high=no_items,size=1)
#       list_label.append(temp)
#       a = np.zeros(no_items)
#       np.put(a,temp,1)
#       y_train.append(a)
#   y_train = np.asarray(y_train)
#   return user, item, y_train, list_label

def mrr(gt_items, pred_items):
    for index,item in enumerate(pred_items):
        if item in gt_items:
            return 1/(index+1)
    return 0.0

# user, item, y_train, list_label = get_batch(batch_size=batch_size)
# print(user.shape,item.shape,y_train.shape)
# print(user[:5])
# print(item[:5])
# print(y_train[:5])
# sess = tf.Session()
# sess.run(tf.initialize_all_variables())
# print("START LOSS: {}".format(sess.run([loss],feed_dict={user_id:user,item_session:item,y:y_train})))
# for i in range(epochs):
#     sess.run([optimizer],feed_dict={user_id:user,item_session:item,y:y_train})

#     result_epoch = i
#     result_loss = sess.run([loss],feed_dict={user_id:user,item_session:item,y:y_train})
#     result_acc = sess.run([accuracy],feed_dict={user_id:user,item_session:item,y:y_train})
    
#     res = sess.run([dense_2],feed_dict={user_id:user,item_session:item,y:y_train})
#     mrr_score = 0.0
#     for i in range(len(res[0])):
#         temp = np.argpartition(res[0][i], -10)[-10:]
#         mrr_score += mrr(temp,list_label[i])


#     sys.stdout.write("\rEpochs: {}, Loss: {}, Accuracy: {}, MRR score: {}".format(result_epoch, result_loss, result_acc, mrr_score/len(res[0])))

losses = []
accs = []


sess = tf.Session()
sess.run(tf.initialize_all_variables())
#print("START LOSS: {}".format(sess.run([loss],feed_dict={user_id:list_id,item_session:list_session,y:list_label})))
for i in range(epochs):
    mrr_score = 0.0
    result_loss = 0.0
    result_acc = 0.0
    for j in range(list_id.shape[0]//batch_size):
        y_temp = []
        for temp in list_label[j*batch_size:(j+1)*batch_size]:
            a = np.zeros(no_items)
            np.put(a,temp,1)
            y_temp.append(a)
        y_temp = np.asarray(y_temp)
        sess.run([optimizer],feed_dict={user_id:list_id[j*batch_size:(j+1)*batch_size],item_session:list_session[j*batch_size:(j+1)*batch_size],y:y_temp})

        result_loss += sess.run([loss],feed_dict={user_id:list_id[j*batch_size:(j+1)*batch_size],item_session:list_session[j*batch_size:(j+1)*batch_size],y:y_temp})[0]
        result_acc += sess.run([accuracy],feed_dict={user_id:list_id[j*batch_size:(j+1)*batch_size],item_session:list_session[j*batch_size:(j+1)*batch_size],y:y_temp})[0]
        
        res2 = sess.run([dense_2],feed_dict={user_id:list_id[j*batch_size:(j+1)*batch_size],item_session:list_session[j*batch_size:(j+1)*batch_size],y:y_temp})
        for g in range(len(res2[0])):
            temp = np.argpartition(res2[0][g], -10)[-10:]
            mrr_score += mrr([np.argmax(y_temp[g])],temp)
    losses.append(result_loss)
    accs.append(result_acc/(list_id.shape[0]))
    sys.stdout.write("\rEpochs: {}, Loss: {}, Accuracy: {}, MRR score: {}".format(i, result_loss, result_acc/(list_id.shape[0]), mrr_score/(list_id.shape[0])))

#import matplotlib.pyplot as plt

#plt.plot(losses)
#plt.title('model loss')
#plt.ylabel('loss')
#plt.xlabel('epoch')
#plt.show()
#plt.plot(accs*100)

